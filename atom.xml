<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ocean of Yogurt</title>
  
  <subtitle>It would be perfect if it works (≡•̀·̯•́≡)</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://lightingghost.github.io/"/>
  <updated>2018-04-29T19:53:44.000Z</updated>
  <id>https://lightingghost.github.io/</id>
  
  <author>
    <name>Zhenpeng Zhou</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Debug cpp Modules Imported by Python</title>
    <link href="https://lightingghost.github.io/2018/04/29/debug-python-c-module/"/>
    <id>https://lightingghost.github.io/2018/04/29/debug-python-c-module/</id>
    <published>2018-04-29T19:40:06.000Z</published>
    <updated>2018-04-29T19:53:44.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Debug-cpp-Modules-Imported-by-Python"><a href="#Debug-cpp-Modules-Imported-by-Python" class="headerlink" title="Debug cpp Modules Imported by Python"></a>Debug cpp Modules Imported by Python</h1><h1 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h1><p>I found a bug when building my model with <code>mxnet</code>, see <a href="https://github.com/apache/incubator-mxnet/issues/10220" target="_blank" rel="noopener">here</a>. It seems <code>mxnet</code> people are busy doing other things, therefore I have to solve this by myself. When I am trying to debug <code>mxnet</code> package using <code>lldb</code> with the following command:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lldb -- python test.py</span><br></pre></td></tr></table></figure><p>and the following command to set a breakpoint in file:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b file:line</span><br></pre></td></tr></table></figure><p><code>lldb</code> cannot successfully set the breakpoint. It seems the python module is not loaded at that time and <code>lldb</code> cannot find the breakpoint.</p><h1 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h1><p>If the problem is on the module loading, we can set a breakpoint in python using </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pdb.set_trace()</span><br></pre></td></tr></table></figure><p>and open another term, attach <code>lldb</code> to the python process using </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">attach --pid XXXX</span><br><span class="line"><span class="built_in">continue</span></span><br></pre></td></tr></table></figure><p>then set the breakpoint, using the absolute path</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b \Users\odin\<span class="built_in">local</span>\mxnet\src\imperative\imperative.cc:300</span><br></pre></td></tr></table></figure><p>and continue in <code>pdb</code>.</p><p>The breakpoint is then successfully set.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Debug-cpp-Modules-Imported-by-Python&quot;&gt;&lt;a href=&quot;#Debug-cpp-Modules-Imported-by-Python&quot; class=&quot;headerlink&quot; title=&quot;Debug cpp Modules Im
      
    
    </summary>
    
      <category term="Memo" scheme="https://lightingghost.github.io/categories/Memo/"/>
    
    
      <category term="memo" scheme="https://lightingghost.github.io/tags/memo/"/>
    
      <category term="lldb" scheme="https://lightingghost.github.io/tags/lldb/"/>
    
      <category term="python" scheme="https://lightingghost.github.io/tags/python/"/>
    
      <category term="cpp" scheme="https://lightingghost.github.io/tags/cpp/"/>
    
  </entry>
  
  <entry>
    <title>Benchmarks of MXNet and Gluon</title>
    <link href="https://lightingghost.github.io/2017/12/28/mxnet-benchmark/"/>
    <id>https://lightingghost.github.io/2017/12/28/mxnet-benchmark/</id>
    <published>2017-12-28T20:37:38.000Z</published>
    <updated>2017-12-29T01:14:36.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Benchmarks-of-MXNet-and-Gluon"><a href="#Benchmarks-of-MXNet-and-Gluon" class="headerlink" title="Benchmarks of MXNet and Gluon"></a>Benchmarks of MXNet and Gluon</h1><p>This is a performance comparison of native MXNet and Gluon</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p><a href="https://mxnet.incubator.apache.org" target="_blank" rel="noopener">MXNet</a> is a deep learning framework supported by Amazon. I have had experience with MXNet in the year of 2014. At that time, the documentation of MXNet was far from satisfactory, and the amount of operators supported was not small. However, the performance of MXNet in terms of training speed and memory usage outperformed almost all others at that time. (I am talking about you, tensorflow 0.1 :-P)</p><p>Recently, MXNet introduced <a href="http://gluon.mxnet.io/index.html" target="_blank" rel="noopener">Gluon</a>, which offers high-level abstractions for predefined layers, loss functions, and optimizers. </p><p>I am trying to adapt MXNet / Gluon for my next project. But I want to persue the best speed and memory efficiency, therefore I am looking for the answer that whether I should stick with native mxnet or try the new gluon</p><h2 id="Gluon"><a href="#Gluon" class="headerlink" title="Gluon"></a>Gluon</h2><p> There a three base block classes in Gluon, <code>Block</code>, <code>HybridBlock</code>, and <code>SymbolBlock</code>. <code>SymbolBlock</code> seems to provide a wrap outside the original mxnet <code>symbol</code> API. <code>Block</code> is the new imperative programming API, while <code>HybridBlock</code> provide more flexibility: it is similar to the <code>Block</code>, but can be <code>hybridize()</code> to make a symbolic computational graph, which provides a better performance.</p><h2 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h2><p>A natural questions is, what is the overheading of the wrap outside MXNet? I am going to compare the performance of native mxnet, gluon SymbolBlock, hybridized HybrideBlock, and Block on diffferent network architectures.</p><p>The code was <a href="https://github.com/lightingghost/mxnet_benchmark" target="_blank" rel="noopener">here</a>. forward and backward were repeated 100 times for average.</p><p>The hardware and software platforms are:</p><p>macOS 10.13, CUDA 9.0 CuDNN 7.0</p><p>Titan Xp, i7-4790K, 32G</p><h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><table><thead><tr><th>Framework</th><th style="text-align:center">Time (ms)</th></tr></thead><tbody><tr><td>native mxnet</td><td style="text-align:center">41.2</td></tr><tr><td>gluon SymbolBlock</td><td style="text-align:center">40.8</td></tr><tr><td>gluon HybridBlock</td><td style="text-align:center">36.8</td></tr><tr><td>gluon HybridBlock (hybridized)</td><td style="text-align:center">36.8</td></tr><tr><td>gluon Block</td><td style="text-align:center">36.9</td></tr></tbody></table><p>It seems the network structure is too simple (sometime naive) to show the difference. I am going to test more complex structures.</p><h3 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h3><table><thead><tr><th>Framework</th><th style="text-align:center">Time (ms)</th></tr></thead><tbody><tr><td>native mxnet</td><td style="text-align:center">236.6</td></tr><tr><td>gluon SymbolBlock</td><td style="text-align:center">255.1</td></tr><tr><td>gluon HybridBlock</td><td style="text-align:center">270.4</td></tr><tr><td>gluon HybridBlock (hybridized)</td><td style="text-align:center">222.7</td></tr><tr><td>gluon Block</td><td style="text-align:center">272.4</td></tr></tbody></table><p>Interestingly, the hybridized HybridBlock gives the best performance in both cases.</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>The performance lost due to the wrapping of gluon is minimal.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Benchmarks-of-MXNet-and-Gluon&quot;&gt;&lt;a href=&quot;#Benchmarks-of-MXNet-and-Gluon&quot; class=&quot;headerlink&quot; title=&quot;Benchmarks of MXNet and Gluon&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="Memo" scheme="https://lightingghost.github.io/categories/Memo/"/>
    
    
      <category term="deep learning" scheme="https://lightingghost.github.io/tags/deep-learning/"/>
    
      <category term="benchmark" scheme="https://lightingghost.github.io/tags/benchmark/"/>
    
      <category term="mxnet" scheme="https://lightingghost.github.io/tags/mxnet/"/>
    
  </entry>
  
  <entry>
    <title>The Problem of Subscript in Hexo with Mathjax</title>
    <link href="https://lightingghost.github.io/2017/12/26/math-hexo/"/>
    <id>https://lightingghost.github.io/2017/12/26/math-hexo/</id>
    <published>2017-12-27T06:15:20.000Z</published>
    <updated>2018-04-29T19:43:09.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="The-Problem-of-Subscript-in-Hexo-with-Mathjax"><a href="#The-Problem-of-Subscript-in-Hexo-with-Mathjax" class="headerlink" title="The Problem of Subscript in Hexo with Mathjax "></a>The Problem of Subscript in Hexo with Mathjax </h1><h1 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h1><p>When writing equations with hexo, there are a lot of problems with its math engine. The specific problem that I came across is that the inline formula of </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$\&#123;P_sa\&#125;$</span><br></pre></td></tr></table></figure><p>will not render correctly.</p><h1 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h1><h2 id="LaTeX"><a href="#LaTeX" class="headerlink" title="LaTeX"></a>LaTeX</h2><p>The formula was rendered correctly on $\LaTeX$. So I did not make it wrong in the formula.</p><h2 id="Mathjax"><a href="#Mathjax" class="headerlink" title="Mathjax"></a>Mathjax</h2><p>Then I thought the problem might be with <code>mathjax</code>. However, when I tested the same formula directly written in html, it worked perfectly.</p><h2 id="Hexo"><a href="#Hexo" class="headerlink" title="Hexo"></a>Hexo</h2><p>When I checked the html page generated by hexo, I found out that the formula was rendered as:</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span>$\&#123;P<span class="tag">&lt;<span class="name">em</span>&gt;</span>sa\&#125;$......<span class="tag">&lt;<span class="name">\em</span>&gt;</span></span><br></pre></td></tr></table></figure><p>The problem is, markdown will use not only  <code>*asterisks*</code> for emphasis, but also <code>_underscores_</code>. What an  <del>stupid</del> interesting  design!</p><h1 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h1><p>In the file of <code>node_modules/marked/lib/marked.js</code>:</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">link: <span class="regexp">/^!?\[(inside)\]\(href\)/</span>,</span><br><span class="line">reflink: <span class="regexp">/^!?\[(inside)\]\s*\[([^\]]*)\]/</span>,</span><br><span class="line">nolink: <span class="regexp">/^!?\[((?:\[[^\]]*\]|[^\[\]])*)\]/</span>,</span><br><span class="line">strong: <span class="regexp">/^__([\s\S]+?)__(?!_)|^\*\*([\s\S]+?)\*\*(?!\*)/</span>,</span><br><span class="line">em: <span class="regexp">/^\b_((?:[^_]|__)+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br><span class="line">code: <span class="regexp">/^(`+)([\s\S]*?[^`])\1(?!`)/</span>,</span><br><span class="line">br: <span class="regexp">/^ &#123;2,&#125;\n(?!\s*$)/</span>,</span><br><span class="line">del: noop,</span><br><span class="line">text: <span class="regexp">/^[\s\S]+?(?=[\\&lt;!\[_*`]| &#123;2,&#125;\n|$)/</span></span><br></pre></td></tr></table></figure><p>Change the line</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">em: <span class="regexp">/^\b_((?:[^_]|__)+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br></pre></td></tr></table></figure><p>to </p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">em:  <span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br></pre></td></tr></table></figure><p>This will disable the effort of trying to match <code>_</code> for the regular expression engine.</p><p>Now it works.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;The-Problem-of-Subscript-in-Hexo-with-Mathjax&quot;&gt;&lt;a href=&quot;#The-Problem-of-Subscript-in-Hexo-with-Mathjax&quot; class=&quot;headerlink&quot; title=&quot;Th
      
    
    </summary>
    
      <category term="Memo" scheme="https://lightingghost.github.io/categories/Memo/"/>
    
    
      <category term="memo" scheme="https://lightingghost.github.io/tags/memo/"/>
    
      <category term="hexo" scheme="https://lightingghost.github.io/tags/hexo/"/>
    
      <category term="mathjax" scheme="https://lightingghost.github.io/tags/mathjax/"/>
    
  </entry>
  
  <entry>
    <title>Optimizing Chemical Reactions with Deep Reinforcement Learning</title>
    <link href="https://lightingghost.github.io/2017/12/26/chemopt-intro/"/>
    <id>https://lightingghost.github.io/2017/12/26/chemopt-intro/</id>
    <published>2017-12-27T04:26:06.000Z</published>
    <updated>2017-12-29T23:09:38.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Optimizing-Chemical-Reactions-with-Deep-Reinforcement-Learning"><a href="#Optimizing-Chemical-Reactions-with-Deep-Reinforcement-Learning" class="headerlink" title="Optimizing Chemical Reactions with Deep Reinforcement Learning"></a>Optimizing Chemical Reactions with Deep Reinforcement Learning</h1><p><a href="http://pubs.acs.org/doi/full/10.1021/acscentsci.7b00492" target="_blank" rel="noopener">ACS Cent. Sci., 2017, 3 (12), pp 1337–1344</a></p><p><img src="/img/chemopt/robo_chem.jpg" alt="TOC"></p><h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>There has been various efforts on applying the idea of machine learning and artificial intelligence on the field of physical science. In terms of chemistry or biological science, some of the major interests are:</p><ul><li>Predictions disease states by applying classification / regression models on experimental data.</li><li>Using a neural network to theoretically predict properties of molecules. And, one step further, to design new molecules.</li><li>Predict the product of a reaction. And retro-synthesis analysis.</li></ul><p>In this post, I will briefly introduction a recent work of applying the decision-making framework to solve real world problems in chemistry, specifically chemical reactions.</p><h1 id="Common-Ways-to-Optimize-Chemical-Reactions"><a href="#Common-Ways-to-Optimize-Chemical-Reactions" class="headerlink" title="Common Ways to Optimize Chemical Reactions"></a>Common Ways to Optimize Chemical Reactions</h1><p>There have been various attempts to use automated algorithms to optimize chemical reactions. For example:</p><ul><li>Nelder-Mead Simplex Method</li><li>Stable Noisy Optimization by Branch and Fit (SNOBFIT)</li></ul><p>Unfortunately, the most common practice among chemist to optimize a reaction is the one variable at a time (OVAT) method, which is changing a single experimental condition at a time while fixing all the others. This method often miss the optimal condition.</p><h1 id="Our-method"><a href="#Our-method" class="headerlink" title="Our method"></a>Our method</h1><h2 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h2><p>A reaction can be viewed as a system taking multiple inputs (experimental conditions) and providing one desired output. Example inputs include temperature, solvent composition, pH, catalyst, and time. Example outputs include product yield, selectivity, purity, and cost. The reaction can be modeled by a function $r = R(s)$, where $s$ stands for the experimental conditions and $r$ denotes the objective, say, the yield. </p><p>There are two properties makes optimizing chemical reactions a hard problem:</p><ul><li><p>chemical reactions are expensive and time-consuming to conduct</p></li><li><p>the outcome can vary largely, which is caused in part by measurement errors. </p></li></ul><h2 id="A-Decision-Making-Framwork"><a href="#A-Decision-Making-Framwork" class="headerlink" title="A Decision-Making Framwork"></a>A Decision-Making Framwork</h2><p>We can formulate the process of finding the optimal reaction condition as a decision making problem, i.e., a Markov decision process (MDP) of $(\mathcal{S}, \mathcal{A}, \{P_{sa}\}, \mathcal{R})$:</p><ul><li><p>$\mathcal{S}$ denotes the set of <strong>states</strong> $s$. In the context of reaction optimization, $\mathcal{S}$ is the set of all possible combinations of experimental conditions.</p></li><li><p>$\mathcal{A}$ denotes the set of <strong>actions</strong> $a$. In the context of reaction optimization, $\mathcal{A}$ is the set of all changes that can be made to the experimental conditions, for example, increasing the temperature by 10 °C and so forth.</p></li><li><p>$ \{P_{sa} \} $  denotes the state transition probabilities. Concretely, $P_{sa}$ specifies the probability of transiting from $s$ to another state with action $a$. In the context of a chemical reaction, $P_{sa}$ specifies to what experimental conditions the reaction will move if we decide to make a change a to the experimental condition $s$. Intuitively, $P_{sa}$ measures the inaccuracy when operating the instrument. For example, the action of increasing the temperature by 10 °C may result in a temperature increase of 9.5–10.5 °C.</p></li><li><p>$\mathcal{R}$ denotes the reward function of state $s$ and action $a$. In the environment of a reaction, the reward $r$ is only a function of state $s$, i.e., a certain experimental condition $s$ (state) is mapped to yield $r$ (reward) by the reward function $r = R(s)$.</p></li></ul><p>Our goal is to find a <strong>policy</strong> for this decision making problem. In the context of chemical reactions, the policy refers to the algorithm that interacts with the chemical reaction to obtain the current reaction condition and reaction yield, from which the next experimental conditions are chosen. Rigorously, we define the policy as the function $\pi$, which maps from the current experimental condition $s_t$ and history of the experiment record $\mathcal{H}_t$ to the next experimental condition, that is,</p><p>$$ s_{t+1} = \pi (s_t,\mathcal{H}_t)$$</p><p>where $\mathcal{H}_t$ is the history, and $t$ records the number of steps we have taken in reaction optimization.</p><p>Here, we use a different defination of policy compared to the traditional one of $a = \pi (s)$. This is for simplicity and it has no conflict with the traditional policy defination, because in chemical reaction, we can define the action as the vector difference between two states: $a_t = s_{t+1} - s_t$.</p><p>Intuitively, the optimization procedure can be explained as follows: We iteratively conduct an experiment under a specific experimental condition and record the yield. Then the policy function makes use of all the history of experimental record (what condition led to what yield) and tells us what experimental condition we should try next. This procedure is illustrated in Figure 1.</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/chemopt/scheme.png" alt="Figure 1. Visualization of the Model Unrolled over Three Time Steps" title="">                </div>                <div class="image-caption">Figure 1. Visualization of the Model Unrolled over Three Time Steps</div>            </figure><h2 id="Recurrent-Neural-Network-as-Policy"><a href="#Recurrent-Neural-Network-as-Policy" class="headerlink" title="Recurrent Neural Network as Policy"></a>Recurrent Neural Network as Policy</h2><p>We employs the recurrent neural network (RNN) to fit the policy function $\pi$ under the settings of chemical reactions. A RNN takes a similar form of the policy function can be written as follows:</p><p>$$s_{t+1},h_{t+1}=\mathrm{RNN}_\theta(s_t,r_t,h_t)$$</p><p>where at time step $t$, $h_t$ is the hidden state to model the history $\mathcal{H}_t$, $s_t$ denotes the state of reaction condition, and $r_t$ is the yield (reward) of reaction outcome. The policy of RNN maps the inputs at time step $t$ to outputs at time step $t + 1$. </p><p>And the loss function is defined as the observed improvement:</p><p>$$l(\theta) = -\sum_{t=1}^{T}\left(r_t-\max_{i&lt;t}r_i\right)$$</p><p>The term inside the parentheses measures the improvement we can achieve by iteratively conducting different experiments. The loss function encourages reaching the optimal condition faster, in order to address the problem that chemical reactions are expensive and time-consuming to conduct.</p><h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><h2 id="Simulated-Reactions"><a href="#Simulated-Reactions" class="headerlink" title="Simulated Reactions"></a>Simulated Reactions</h2><p>As mentioned earlier, chemical reactions are time-consuming to evaluate. Although our model can greatly accelerate the procedure, we still propose to first train the model on simulated reactions. A set of mixture Gaussian density functions is used as the simulated reactions environment $r = R(s)$. A Gaussian error term is added to the function to model the large variance property of chemical reaction measurements. The mock reactions can be written as:</p><p>$$ y = \sum_{i=1}^N c_i (2\pi)^{-k/2}|\Sigma_i|^{-1/2} \exp\left(-\frac{1}{2}(x-\mu_i)^T\Sigma_i^{-1}(x-\mu_i)\right) +\varepsilon$$</p><p>where $c_i$ is the coefficient, $\mu_i$ is the mean, and $\Sigma_i$ is the covariance of a multivariate Gaussian distribution; $k$ is the dimension of the variables. $\varepsilon$ is the error term, which is a random variable drawn from a Gaussian distribution with mean $0$ and variance $\sigma^2$.</p><p>The motivation for using a mixture of Gaussian density functions comes from the idea that they can be used to approximate arbitrarily close all continuous functions on a bounded set. We assume that the response surface for most reactions is a continuous function, which can be well approximated by a mixture of Gaussian density functions. Besides, a mixture of Gaussian density functions often has multiple local minima. The rationale behind this is that the response surface of a chemical reaction may also have multiple local optima. As a result, we believe a mixture of Gaussian density functions can be a good class of function to simulate real reactions.</p><p>We compared our model with several state-of-the-art blackbox optimization algorithms of covariance matrix adaption–evolution strategy (CMA-ES), Nelder–Mead simplex method, and stable noisy optimization by branch and fit (SNOBFIT) on another set of mixture Gaussian density functions that are unseen during training. This comparison is a classic approach for model evaluation in machine learning. We use “regret” to evaluate the performance of the models. The regret is defined as</p><p>$$ \mathrm{regret}(t) = \max_s R(s) - r_t$$</p><p>and it measures the gap between the current reward and the largest reward that is possible. Lower regret indicates better optimization. </p><p>Figure 2A shows the average regret versus time steps of the two algorithms from which we see that our model outperforms CMA-ES significantly by reaching a lower regret value in fewer steps.</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/chemopt/cmp.png" alt="Figure 2. (A) Comparison of average regret of CMA-ES, Nelder–Mead simplex method, SNOBFIT, and DRO. The average regret is calculated as the average regret on 1000 random nonconvex functions. (B) The observed regret of 10 random nonconvex functions in which each line is the regret of one function." title="">                </div>                <div class="image-caption">Figure 2. (A) Comparison of average regret of CMA-ES, Nelder–Mead simplex method, SNOBFIT, and DRO. The average regret is calculated as the average regret on 1000 random nonconvex functions. (B) The observed regret of 10 random nonconvex functions in which each line is the regret of one function.</div>            </figure><h2 id="Randomized-Policy-for-Deep-Exploration"><a href="#Randomized-Policy-for-Deep-Exploration" class="headerlink" title="Randomized Policy for Deep Exploration"></a>Randomized Policy for Deep Exploration</h2><p>Although our model optimizes nonconvex functions faster than CMA-ES, we observe that it sometimes get stuck in a local maximum (Figure 2B) because of the deterministic <em>greedy</em> policy, where greedy means making the locally optimal choice at each stage without exploration. In the context of reaction optimization, a greedy policy will stick to one reaction condition if it is better than any other conditions observed. However, the greedy policy will get trapped in a local optimum, failing to explore some regions in the space of experimental conditions, which may contain a better reaction condition that we are looking for. To further accelerate the optimization procedure in this aspect, we proposed a randomized exploration regime to explore different experimental conditions, in which randomization means drawing the decision randomly from a certain probability distribution. This idea came from the work of Van Roy and co-workers, which showed that deep exploration can be achieved from randomized value function estimates. The stochastic policy also addresses the problem of randomness in chemical reactions.<br>A stochastic recurrent neural network was used to model a randomized policy, which can be written as</p><p>$$ h_{t+1},\Sigma_{t+1},\mu_{t+1}=\mathrm{RNN}<em>{\theta}\left(h</em>{t},r_{t},s_{t}\right)$$</p><p>$$s_{t+1}\sim\mathcal{N}\left(\mu_{t+1},\Sigma_{t+1}\right)$$</p><p>Similar to the notations introduced before, the RNN is used to generate the mean $\mu_{t+1}$, and the covariance matrix $\Sigma_{t+1}$; the next state $s_{t+1}$ is then drawn from a multivariate Gaussian distribution of $\mathcal{N}(\mu_{t+1},\Sigma_{t+1})$ at time step $t + 1$. This approach achieved deep exploration in a computationally efficient way.</p><p>Figure 3 compares between the greedy policy and the randomized policy on another group of simulated reactions. Although the randomized policy was slightly slower, it arrives to a better function value owing to its more efficient exploration strategy. Comparing the randomized policy with a deterministic one, the average regret was improved from 0.062 to 0.039, which shows a better chance of finding the global optimal conditions.</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/chemopt/srnn.png" alt="Figure 3. Comparison of deterministic policy and randomized policy in the model of DRO." title="">                </div>                <div class="image-caption">Figure 3. Comparison of deterministic policy and randomized policy in the model of DRO.</div>            </figure><h2 id="Optimization-of-Real-Reactions"><a href="#Optimization-of-Real-Reactions" class="headerlink" title="Optimization of Real Reactions"></a>Optimization of Real Reactions</h2><p>We carried out four experiments in microdroplets and recorded the production yield: The Pomeranz–Fritsch synthesis of isoquinoline, Friedländer synthesis of a substituted quinoline, the synthesis of ribose phosphate, and the reaction between 2,6-dichlorophenolindophenol (DCIP) and ascorbic acid. Our model outperformed the other two methods by reaching a higher yield in fewer steps. In both reactions, the model found the optimal condition within 40 steps, with the total time of 30 min required to optimize a reaction. (Figure 4)</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/chemopt/4rxn.png" alt="Figure 4. Performance comparison of CMA-ES, DRO, and OVAT methods on the microdroplet reaction of (A) Pomeranz–Fritsch synthesis of isoquinoline, (B) Friedländer synthesis of a substituted quinoline, (C) synthesis of ribose phosphate, and (D) the reaction between DCIP and ascorbic acid. The signal intensity can be converted into reaction yield with calibration." title="">                </div>                <div class="image-caption">Figure 4. Performance comparison of CMA-ES, DRO, and OVAT methods on the microdroplet reaction of (A) Pomeranz–Fritsch synthesis of isoquinoline, (B) Friedländer synthesis of a substituted quinoline, (C) synthesis of ribose phosphate, and (D) the reaction between DCIP and ascorbic acid. The signal intensity can be converted into reaction yield with calibration.</div>            </figure><h2 id="Learning-for-Better-Optimization"><a href="#Learning-for-Better-Optimization" class="headerlink" title="Learning for Better Optimization"></a>Learning for Better Optimization</h2><p>We also observed that the algorithm is capable of learning while optimizing on real experiments. In other words, each time running a similar or even dissimilar reactions will improve the policy. To demonstrate this point, the model was first trained on the reaction of the Pomeranz–Fritsch synthesis of isoquinoline and then tested on the reaction of the Friedländer synthesis of substituted quinoline. Figure 5 compares the performance of the model before and after training. The policy after training showed a better performance by reaching a higher yield at a faster speed.</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/img/chemopt/ll.png" alt="Figure 5. (A) The performance on Friedländer synthesis of DRO before and after training on the Pomeranz–Fritsch synthesis. (B) The performance on ribose phosphate synthesis of DRO before and after training on the Pomeranz–Fritsch and Friedländer syntheses." title="">                </div>                <div class="image-caption">Figure 5. (A) The performance on Friedländer synthesis of DRO before and after training on the Pomeranz–Fritsch synthesis. (B) The performance on ribose phosphate synthesis of DRO before and after training on the Pomeranz–Fritsch and Friedländer syntheses.</div>            </figure><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>Our model showed strong generalizability in two ways: First, based on optimization of a large family of functions, our optimization goal can be not only yield but also selectivity, purity, or cost, because all of them can be modeled by a function of experimental parameters. Second, a wide range of reactions can be accelerated by $10^3$ to $10^6$ times in microdroplets. Showing that a microdroplet reaction can be optimized in 30 min by our model, we therefore propose that a large class of reactions can be optimized by our model. The wide applicability of our model suggests it to be useful in both academic research and industrial production.</p><h1 id="Code-Availability"><a href="#Code-Availability" class="headerlink" title="Code Availability"></a>Code Availability</h1><p>The code of this project can be found <a href="https://github.com/lightingghost/chemopt" target="_blank" rel="noopener">here</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Optimizing-Chemical-Reactions-with-Deep-Reinforcement-Learning&quot;&gt;&lt;a href=&quot;#Optimizing-Chemical-Reactions-with-Deep-Reinforcement-Lear
      
    
    </summary>
    
      <category term="Project" scheme="https://lightingghost.github.io/categories/Project/"/>
    
    
      <category term="deep learning" scheme="https://lightingghost.github.io/tags/deep-learning/"/>
    
      <category term="project" scheme="https://lightingghost.github.io/tags/project/"/>
    
      <category term="reinforcement learning" scheme="https://lightingghost.github.io/tags/reinforcement-learning/"/>
    
      <category term="optimization" scheme="https://lightingghost.github.io/tags/optimization/"/>
    
      <category term="chemistry" scheme="https://lightingghost.github.io/tags/chemistry/"/>
    
  </entry>
  
</feed>
