{"meta":{"title":"Ocean of Yogurt","subtitle":"It would be perfect if it works (≡•̀·̯•́≡)","description":null,"author":"Zhenpeng Zhou","url":"https://lightingghost.github.io"},"pages":[{"title":"Categories","date":"2017-12-27T00:12:55.000Z","updated":"2017-12-27T00:29:42.000Z","comments":false,"path":"categories/index.html","permalink":"https://lightingghost.github.io/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2017-12-27T00:08:03.000Z","updated":"2017-12-27T00:29:25.000Z","comments":false,"path":"tags/index.html","permalink":"https://lightingghost.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Debug cpp Modules Imported by Python","slug":"debug-python-c-module","date":"2018-04-29T19:40:06.000Z","updated":"2018-04-29T19:54:05.000Z","comments":true,"path":"2018/04/29/debug-python-c-module/","link":"","permalink":"https://lightingghost.github.io/2018/04/29/debug-python-c-module/","excerpt":"","text":"Debug cpp Modules Imported by PythonProblemI found a bug when building my model with mxnet, see here. It seems mxnet people are busy doing other things, therefore I have to solve this by myself. When I am trying to debug mxnet package using lldb with the following command: 1lldb -- python test.py and the following command to set a breakpoint in file: 1b file:line lldb cannot successfully set the breakpoint. It seems the python module is not loaded at that time and lldb cannot find the breakpoint. SolutionIf the problem is on the module loading, we can set a breakpoint in python using 1pdb.set_trace() and open another term, attach lldb to the python process using 12attach --pid XXXXcontinue then set the breakpoint, using the absolute path 1b \\Users\\odin\\local\\mxnet\\src\\imperative\\imperative.cc:300 and continue in pdb. The breakpoint is then successfully set.","categories":[{"name":"Memo","slug":"Memo","permalink":"https://lightingghost.github.io/categories/Memo/"}],"tags":[{"name":"memo","slug":"memo","permalink":"https://lightingghost.github.io/tags/memo/"},{"name":"lldb","slug":"lldb","permalink":"https://lightingghost.github.io/tags/lldb/"},{"name":"python","slug":"python","permalink":"https://lightingghost.github.io/tags/python/"},{"name":"cpp","slug":"cpp","permalink":"https://lightingghost.github.io/tags/cpp/"}]},{"title":"Benchmarks of MXNet and Gluon","slug":"mxnet-benchmark","date":"2017-12-28T20:37:38.000Z","updated":"2017-12-29T01:14:36.000Z","comments":true,"path":"2017/12/28/mxnet-benchmark/","link":"","permalink":"https://lightingghost.github.io/2017/12/28/mxnet-benchmark/","excerpt":"","text":"Benchmarks of MXNet and GluonThis is a performance comparison of native MXNet and Gluon IntroductionMXNet is a deep learning framework supported by Amazon. I have had experience with MXNet in the year of 2014. At that time, the documentation of MXNet was far from satisfactory, and the amount of operators supported was not small. However, the performance of MXNet in terms of training speed and memory usage outperformed almost all others at that time. (I am talking about you, tensorflow 0.1 :-P) Recently, MXNet introduced Gluon, which offers high-level abstractions for predefined layers, loss functions, and optimizers. I am trying to adapt MXNet / Gluon for my next project. But I want to persue the best speed and memory efficiency, therefore I am looking for the answer that whether I should stick with native mxnet or try the new gluon Gluon There a three base block classes in Gluon, Block, HybridBlock, and SymbolBlock. SymbolBlock seems to provide a wrap outside the original mxnet symbol API. Block is the new imperative programming API, while HybridBlock provide more flexibility: it is similar to the Block, but can be hybridize() to make a symbolic computational graph, which provides a better performance. ComparisonA natural questions is, what is the overheading of the wrap outside MXNet? I am going to compare the performance of native mxnet, gluon SymbolBlock, hybridized HybrideBlock, and Block on diffferent network architectures. The code was here. forward and backward were repeated 100 times for average. The hardware and software platforms are: macOS 10.13, CUDA 9.0 CuDNN 7.0 Titan Xp, i7-4790K, 32G AlexNet Framework Time (ms) native mxnet 41.2 gluon SymbolBlock 40.8 gluon HybridBlock 36.8 gluon HybridBlock (hybridized) 36.8 gluon Block 36.9 It seems the network structure is too simple (sometime naive) to show the difference. I am going to test more complex structures. GoogLeNet Framework Time (ms) native mxnet 236.6 gluon SymbolBlock 255.1 gluon HybridBlock 270.4 gluon HybridBlock (hybridized) 222.7 gluon Block 272.4 Interestingly, the hybridized HybridBlock gives the best performance in both cases. ConclusionThe performance lost due to the wrapping of gluon is minimal.","categories":[{"name":"Memo","slug":"Memo","permalink":"https://lightingghost.github.io/categories/Memo/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"https://lightingghost.github.io/tags/deep-learning/"},{"name":"benchmark","slug":"benchmark","permalink":"https://lightingghost.github.io/tags/benchmark/"},{"name":"mxnet","slug":"mxnet","permalink":"https://lightingghost.github.io/tags/mxnet/"}]},{"title":"The Problem of Subscript in Hexo with Mathjax","slug":"math-hexo","date":"2017-12-27T06:15:20.000Z","updated":"2018-04-29T19:43:09.000Z","comments":true,"path":"2017/12/26/math-hexo/","link":"","permalink":"https://lightingghost.github.io/2017/12/26/math-hexo/","excerpt":"","text":"The Problem of Subscript in Hexo with Mathjax ProblemWhen writing equations with hexo, there are a lot of problems with its math engine. The specific problem that I came across is that the inline formula of 1$\\&#123;P_sa\\&#125;$ will not render correctly. TestLaTeXThe formula was rendered correctly on $\\LaTeX$. So I did not make it wrong in the formula. MathjaxThen I thought the problem might be with mathjax. However, when I tested the same formula directly written in html, it worked perfectly. HexoWhen I checked the html page generated by hexo, I found out that the formula was rendered as: 1&lt;p&gt;$\\&#123;P&lt;em&gt;sa\\&#125;$......&lt;\\em&gt; The problem is, markdown will use not only *asterisks* for emphasis, but also _underscores_. What an stupid interesting design! SolutionIn the file of node_modules/marked/lib/marked.js: 123456789link: /^!?\\[(inside)\\]\\(href\\)/,reflink: /^!?\\[(inside)\\]\\s*\\[([^\\]]*)\\]/,nolink: /^!?\\[((?:\\[[^\\]]*\\]|[^\\[\\]])*)\\]/,strong: /^__([\\s\\S]+?)__(?!_)|^\\*\\*([\\s\\S]+?)\\*\\*(?!\\*)/,em: /^\\b_((?:[^_]|__)+?)_\\b|^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,code: /^(`+)([\\s\\S]*?[^`])\\1(?!`)/,br: /^ &#123;2,&#125;\\n(?!\\s*$)/,del: noop,text: /^[\\s\\S]+?(?=[\\\\&lt;!\\[_*`]| &#123;2,&#125;\\n|$)/ Change the line 1em: /^\\b_((?:[^_]|__)+?)_\\b|^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/, to 1em: /^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/, This will disable the effort of trying to match _ for the regular expression engine. Now it works.","categories":[{"name":"Memo","slug":"Memo","permalink":"https://lightingghost.github.io/categories/Memo/"}],"tags":[{"name":"memo","slug":"memo","permalink":"https://lightingghost.github.io/tags/memo/"},{"name":"hexo","slug":"hexo","permalink":"https://lightingghost.github.io/tags/hexo/"},{"name":"mathjax","slug":"mathjax","permalink":"https://lightingghost.github.io/tags/mathjax/"}]},{"title":"Optimizing Chemical Reactions with Deep Reinforcement Learning","slug":"chemopt-intro","date":"2017-12-27T04:26:06.000Z","updated":"2018-04-29T20:09:29.000Z","comments":true,"path":"2017/12/26/chemopt-intro/","link":"","permalink":"https://lightingghost.github.io/2017/12/26/chemopt-intro/","excerpt":"","text":"Optimizing Chemical Reactions with Deep Reinforcement LearningACS Cent. Sci., 2017, 3 (12), pp 1337–1344 OverviewThere has been various efforts on applying the idea of machine learning and artificial intelligence on the field of physical science. In terms of chemistry or biological science, some of the major interests are: Predictions disease states by applying classification / regression models on experimental data. Using a neural network to theoretically predict properties of molecules. And, one step further, to design new molecules. Predict the product of a reaction. And retro-synthesis analysis. In this post, I will briefly introduction a recent work of applying the decision-making framework to solve real world problems in chemistry, specifically chemical reactions. Common Ways to Optimize Chemical ReactionsThere have been various attempts to use automated algorithms to optimize chemical reactions. For example: Nelder-Mead Simplex Method Stable Noisy Optimization by Branch and Fit (SNOBFIT) Unfortunately, the most common practice among chemist to optimize a reaction is the one variable at a time (OVAT) method, which is changing a single experimental condition at a time while fixing all the others. This method often miss the optimal condition. Our methodProblem FormulationA reaction can be viewed as a system taking multiple inputs (experimental conditions) and providing one desired output. Example inputs include temperature, solvent composition, pH, catalyst, and time. Example outputs include product yield, selectivity, purity, and cost. The reaction can be modeled by a function $r = R(s)$, where $s$ stands for the experimental conditions and $r$ denotes the objective, say, the yield. There are two properties makes optimizing chemical reactions a hard problem: chemical reactions are expensive and time-consuming to conduct the outcome can vary largely, which is caused in part by measurement errors. A Decision-Making FramworkWe can formulate the process of finding the optimal reaction condition as a decision making problem, i.e., a Markov decision process (MDP) of $(\\mathcal{S}, \\mathcal{A}, \\{P_{sa}\\}, \\mathcal{R})$: $\\mathcal{S}$ denotes the set of states $s$. In the context of reaction optimization, $\\mathcal{S}$ is the set of all possible combinations of experimental conditions. $\\mathcal{A}$ denotes the set of actions $a$. In the context of reaction optimization, $\\mathcal{A}$ is the set of all changes that can be made to the experimental conditions, for example, increasing the temperature by 10 °C and so forth. $ \\{P_{sa} \\} $ denotes the state transition probabilities. Concretely, $P_{sa}$ specifies the probability of transiting from $s$ to another state with action $a$. In the context of a chemical reaction, $P_{sa}$ specifies to what experimental conditions the reaction will move if we decide to make a change a to the experimental condition $s$. Intuitively, $P_{sa}$ measures the inaccuracy when operating the instrument. For example, the action of increasing the temperature by 10 °C may result in a temperature increase of 9.5–10.5 °C. $\\mathcal{R}$ denotes the reward function of state $s$ and action $a$. In the environment of a reaction, the reward $r$ is only a function of state $s$, i.e., a certain experimental condition $s$ (state) is mapped to yield $r$ (reward) by the reward function $r = R(s)$. Our goal is to find a policy for this decision making problem. In the context of chemical reactions, the policy refers to the algorithm that interacts with the chemical reaction to obtain the current reaction condition and reaction yield, from which the next experimental conditions are chosen. Rigorously, we define the policy as the function $\\pi$, which maps from the current experimental condition $s_t$ and history of the experiment record $\\mathcal{H}_t$ to the next experimental condition, that is, $$ s_{t+1} = \\pi (s_t,\\mathcal{H}_t)$$ where $\\mathcal{H}_t$ is the history, and $t$ records the number of steps we have taken in reaction optimization. Here, we use a different defination of policy compared to the traditional one of $a = \\pi (s)$. This is for simplicity and it has no conflict with the traditional policy defination, because in chemical reaction, we can define the action as the vector difference between two states: $a_t = s_{t+1} - s_t$. Intuitively, the optimization procedure can be explained as follows: We iteratively conduct an experiment under a specific experimental condition and record the yield. Then the policy function makes use of all the history of experimental record (what condition led to what yield) and tells us what experimental condition we should try next. This procedure is illustrated in Figure 1. Figure 1. Visualization of the Model Unrolled over Three Time Steps Recurrent Neural Network as PolicyWe employs the recurrent neural network (RNN) to fit the policy function $\\pi$ under the settings of chemical reactions. A RNN takes a similar form of the policy function can be written as follows: $$s_{t+1},h_{t+1}=\\mathrm{RNN}_\\theta(s_t,r_t,h_t)$$ where at time step $t$, $h_t$ is the hidden state to model the history $\\mathcal{H}_t$, $s_t$ denotes the state of reaction condition, and $r_t$ is the yield (reward) of reaction outcome. The policy of RNN maps the inputs at time step $t$ to outputs at time step $t + 1$. And the loss function is defined as the observed improvement: $$l(\\theta) = -\\sum_{t=1}^{T}\\left(r_t-\\max_{i&lt;t}r_i\\right)$$ The term inside the parentheses measures the improvement we can achieve by iteratively conducting different experiments. The loss function encourages reaching the optimal condition faster, in order to address the problem that chemical reactions are expensive and time-consuming to conduct. ResultsSimulated ReactionsAs mentioned earlier, chemical reactions are time-consuming to evaluate. Although our model can greatly accelerate the procedure, we still propose to first train the model on simulated reactions. A set of mixture Gaussian density functions is used as the simulated reactions environment $r = R(s)$. A Gaussian error term is added to the function to model the large variance property of chemical reaction measurements. The mock reactions can be written as: $$ y = \\sum_{i=1}^N c_i (2\\pi)^{-k/2}|\\Sigma_i|^{-1/2} \\exp\\left(-\\frac{1}{2}(x-\\mu_i)^T\\Sigma_i^{-1}(x-\\mu_i)\\right) +\\varepsilon$$ where $c_i$ is the coefficient, $\\mu_i$ is the mean, and $\\Sigma_i$ is the covariance of a multivariate Gaussian distribution; $k$ is the dimension of the variables. $\\varepsilon$ is the error term, which is a random variable drawn from a Gaussian distribution with mean $0$ and variance $\\sigma^2$. The motivation for using a mixture of Gaussian density functions comes from the idea that they can be used to approximate arbitrarily close all continuous functions on a bounded set. We assume that the response surface for most reactions is a continuous function, which can be well approximated by a mixture of Gaussian density functions. Besides, a mixture of Gaussian density functions often has multiple local minima. The rationale behind this is that the response surface of a chemical reaction may also have multiple local optima. As a result, we believe a mixture of Gaussian density functions can be a good class of function to simulate real reactions. We compared our model with several state-of-the-art blackbox optimization algorithms of covariance matrix adaption–evolution strategy (CMA-ES), Nelder–Mead simplex method, and stable noisy optimization by branch and fit (SNOBFIT) on another set of mixture Gaussian density functions that are unseen during training. This comparison is a classic approach for model evaluation in machine learning. We use “regret” to evaluate the performance of the models. The regret is defined as $$ \\mathrm{regret}(t) = \\max_s R(s) - r_t$$ and it measures the gap between the current reward and the largest reward that is possible. Lower regret indicates better optimization. Figure 2A shows the average regret versus time steps of the two algorithms from which we see that our model outperforms CMA-ES significantly by reaching a lower regret value in fewer steps. Figure 2. (A) Comparison of average regret of CMA-ES, Nelder–Mead simplex method, SNOBFIT, and DRO. The average regret is calculated as the average regret on 1000 random nonconvex functions. (B) The observed regret of 10 random nonconvex functions in which each line is the regret of one function. Randomized Policy for Deep ExplorationAlthough our model optimizes nonconvex functions faster than CMA-ES, we observe that it sometimes get stuck in a local maximum (Figure 2B) because of the deterministic greedy policy, where greedy means making the locally optimal choice at each stage without exploration. In the context of reaction optimization, a greedy policy will stick to one reaction condition if it is better than any other conditions observed. However, the greedy policy will get trapped in a local optimum, failing to explore some regions in the space of experimental conditions, which may contain a better reaction condition that we are looking for. To further accelerate the optimization procedure in this aspect, we proposed a randomized exploration regime to explore different experimental conditions, in which randomization means drawing the decision randomly from a certain probability distribution. This idea came from the work of Van Roy and co-workers, which showed that deep exploration can be achieved from randomized value function estimates. The stochastic policy also addresses the problem of randomness in chemical reactions.A stochastic recurrent neural network was used to model a randomized policy, which can be written as $$ h_{t+1},\\Sigma_{t+1},\\mu_{t+1}=\\mathrm{RNN}_{\\theta}\\left(h_{t},r_{t},s_{t}\\right)$$ $$s_{t+1}\\sim\\mathcal{N}\\left(\\mu_{t+1},\\Sigma_{t+1}\\right)$$ Similar to the notations introduced before, the RNN is used to generate the mean $\\mu_{t+1}$, and the covariance matrix $\\Sigma_{t+1}$; the next state $s_{t+1}$ is then drawn from a multivariate Gaussian distribution of $\\mathcal{N}(\\mu_{t+1},\\Sigma_{t+1})$ at time step $t + 1$. This approach achieved deep exploration in a computationally efficient way. Figure 3 compares between the greedy policy and the randomized policy on another group of simulated reactions. Although the randomized policy was slightly slower, it arrives to a better function value owing to its more efficient exploration strategy. Comparing the randomized policy with a deterministic one, the average regret was improved from 0.062 to 0.039, which shows a better chance of finding the global optimal conditions. Figure 3. Comparison of deterministic policy and randomized policy in the model of DRO. Optimization of Real ReactionsWe carried out four experiments in microdroplets and recorded the production yield: The Pomeranz–Fritsch synthesis of isoquinoline, Friedländer synthesis of a substituted quinoline, the synthesis of ribose phosphate, and the reaction between 2,6-dichlorophenolindophenol (DCIP) and ascorbic acid. Our model outperformed the other two methods by reaching a higher yield in fewer steps. In both reactions, the model found the optimal condition within 40 steps, with the total time of 30 min required to optimize a reaction. (Figure 4) Figure 4. Performance comparison of CMA-ES, DRO, and OVAT methods on the microdroplet reaction of (A) Pomeranz–Fritsch synthesis of isoquinoline, (B) Friedländer synthesis of a substituted quinoline, (C) synthesis of ribose phosphate, and (D) the reaction between DCIP and ascorbic acid. The signal intensity can be converted into reaction yield with calibration. Learning for Better OptimizationWe also observed that the algorithm is capable of learning while optimizing on real experiments. In other words, each time running a similar or even dissimilar reactions will improve the policy. To demonstrate this point, the model was first trained on the reaction of the Pomeranz–Fritsch synthesis of isoquinoline and then tested on the reaction of the Friedländer synthesis of substituted quinoline. Figure 5 compares the performance of the model before and after training. The policy after training showed a better performance by reaching a higher yield at a faster speed. Figure 5. (A) The performance on Friedländer synthesis of DRO before and after training on the Pomeranz–Fritsch synthesis. (B) The performance on ribose phosphate synthesis of DRO before and after training on the Pomeranz–Fritsch and Friedländer syntheses. ConclusionOur model showed strong generalizability in two ways: First, based on optimization of a large family of functions, our optimization goal can be not only yield but also selectivity, purity, or cost, because all of them can be modeled by a function of experimental parameters. Second, a wide range of reactions can be accelerated by $10^3$ to $10^6$ times in microdroplets. Showing that a microdroplet reaction can be optimized in 30 min by our model, we therefore propose that a large class of reactions can be optimized by our model. The wide applicability of our model suggests it to be useful in both academic research and industrial production. Code AvailabilityThe code of this project can be found here","categories":[{"name":"Project","slug":"Project","permalink":"https://lightingghost.github.io/categories/Project/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"https://lightingghost.github.io/tags/deep-learning/"},{"name":"project","slug":"project","permalink":"https://lightingghost.github.io/tags/project/"},{"name":"reinforcement learning","slug":"reinforcement-learning","permalink":"https://lightingghost.github.io/tags/reinforcement-learning/"},{"name":"optimization","slug":"optimization","permalink":"https://lightingghost.github.io/tags/optimization/"},{"name":"chemistry","slug":"chemistry","permalink":"https://lightingghost.github.io/tags/chemistry/"}]}]}